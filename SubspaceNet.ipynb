{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "#Without a GPU, this notebook might take a (very) long time to run, even for small networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/ Innerloop: subspace network\n",
    "ni input neurons (activity X) projecting on no output neurons (activity Y) with weights W.\n",
    "Lateral \"hierarchical\" inhibition within the output layer (Wlat)\n",
    "\n",
    "Parallelized on ns, nd and nr (number of different plasticity rules run at the same time)\n",
    "\n",
    "size(X) = [nr, nd, ns, ni]  \n",
    "size(Y) = [nr, nd, ns, no]  \n",
    "size(W) = [nr, nd, ni, no]  \n",
    "size(Wlat) = [nr, nd, no, no]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Innerloop_SubspaceNet(): \n",
    "    \n",
    "    def __init__(self, inner_loop_params):\n",
    "        self.dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.ns = inner_loop_params['n_samples_per_dataset']\n",
    "        self.ni = len(inner_loop_params['D'])\n",
    "        self.no = inner_loop_params['n_output']\n",
    "        self.ne = inner_loop_params['n_epochs']\n",
    "        self.nd = inner_loop_params['n_datasets']\n",
    "        self.nr = inner_loop_params['n_rules']\n",
    "        self.eta_oja = inner_loop_params['eta_oja']\n",
    "        self.eta_antiHebb = inner_loop_params['eta_antiHebb']\n",
    "        \n",
    "        self.Y = torch.zeros((self.nr, self.nd, self.ns, self.no), dtype=torch.double, device=self.dev)\n",
    "        self.W = torch.zeros((self.nr, self.nd, self.ni, self.no), dtype=torch.double, device=self.dev)\n",
    "        self.Wlat = torch.zeros((self.nr, self.nd, self.no, self.no), dtype=torch.double, device=self.dev)\n",
    "        \n",
    "        #indices in Wlat that are 0 all the time (hierarchical connections to get the pcs in the right order) \n",
    "        self.ind_Wlat_0 = torch.zeros((self.nr, self.nd, self.no, self.no), dtype=torch.bool, device=self.dev)\n",
    "        for oi in range(self.no): #input y\n",
    "                for oo in range(self.no): #output y\n",
    "                    if oi>=oo: \n",
    "                        self.ind_Wlat_0[:,:,oi,oo] = 1\n",
    "        \n",
    "        #blow up handling\n",
    "        self.blow_up =  [self.ne for i in range(self.nr)] #epoch at which the nets blew up (n_ep = no blow up) \n",
    "        self.not_blown = [i for i in range(self.nr)] #dimensions which did not blow up yet\n",
    "        self.a_blow = inner_loop_params['a_blow']\n",
    "        self.b_blow = inner_loop_params['b_blow']\n",
    "        \n",
    "        #input datasets\n",
    "        self.D = inner_loop_params['D']\n",
    "        self.X, self.pcs = generate_datasets_Gaussian(self.ns, self.nd, self.nr, self.ni, self.D, self.dev)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.Y = torch.zeros((self.nr, self.nd, self.ns, self.no), dtype=torch.double, device=self.dev)\n",
    "        \n",
    "        #initialise the weights\n",
    "        aux = 0.1*torch.randn([self.nd, self.ni, self.no], dtype=torch.double, device=self.dev)\n",
    "        self.W = aux.repeat(self.nr,1,1,1)\n",
    "        aux = 0.1*torch.randn([self.nd, self.no, self.no], dtype=torch.double, device=self.dev)\n",
    "        self.Wlat = aux.repeat(self.nr,1,1,1)\n",
    "        \n",
    "        #blow up handling\n",
    "        self.blow_up =  [self.ne for i in range(self.nr)] #epoch at which the nets blew up (n_ep = no blow up) \n",
    "        self.not_blown = [i for i in range(self.nr)] #dimensions which did not blow up yet\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        #feedforward inputs\n",
    "        self.Y = torch.einsum(\"rdsi,rdio->rdso\", self.X, self.W)\n",
    "        #lateral inhibition\n",
    "        self.Y += torch.einsum(\"rdso,rdop->rdsp\", self.Y, self.Wlat) #p=o (einsum notation requires different name)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, A): \n",
    "        A = torch.tensor(A, dtype=torch.double, device=self.dev)\n",
    "        for epoch_num in range(self.ne):\n",
    "            self.forward()\n",
    "            \n",
    "            #forward plasticity rule (weights between layer 1 and 2, target = Oja's rule) \n",
    "            self.W += self.eta_oja*torch.sum(G_forward(A, self.X, self.Y, self.W, self.dev), 2)/self.ns\n",
    "            \n",
    "            #lateral plasticity rule (intra output layer, target = anti Hebbian)\n",
    "            self.Wlat += self.eta_antiHebb*torch.sum(G_lat(A, self.Y, self.Wlat, self.dev), 2)/self.ns\n",
    "            self.Wlat[self.ind_Wlat_0] = 0 #keep hierarchical connections\n",
    "            \n",
    "            #handle blow-ups\n",
    "            if not torch.equal(self.W,self.W): #nan != nan: there is a nan somewhere (ie blow up). Fast to compute\n",
    "                dim_to_remove = []\n",
    "                for r_num in self.not_blown:\n",
    "                    if not torch.equal(self.W[r_num, :, :], self.W[r_num, :, :]):\n",
    "                        self.blow_up[r_num] = epoch_num\n",
    "                        dim_to_remove.append(r_num)\n",
    "                self.not_blown = [x for x in self.not_blown if x not in dim_to_remove]\n",
    "                \n",
    "        for r_num in self.not_blown: #check for \"almost blow ups\" at the end of training (almost nan), which can screw up the grad\n",
    "            dim_to_remove = []\n",
    "            if torch.max(self.W[r_num]).item() > 100000/self.ns:\n",
    "                self.blow_up[r_num] = self.ne - 1\n",
    "                dim_to_remove.append(r_num)\n",
    "        self.not_blown = [x for x in self.not_blown if x not in dim_to_remove]\n",
    "         \n",
    "        \n",
    "    \n",
    "    def score(self, A):\n",
    "        self.reset()\n",
    "        self.train(A)\n",
    "        self.has_blown_up = False\n",
    "        err_mean_pca = [0]*self.nr\n",
    "        for r_num in range(self.nr):\n",
    "            if self.blow_up[r_num] < self.ne: # blow up on this perturbation of A\n",
    "                self.has_blown_up = True\n",
    "                err_mean_pca[r_num] = self.a_blow*(self.ne - self.blow_up[r_num])/(self.ne) + self.b_blow\n",
    "            else:\n",
    "                aux = self.nd*self.no\n",
    "                for d_num in range(self.nd):\n",
    "                    for o_num in range(self.no):\n",
    "                        err_mean_pca[r_num] += (min(torch.norm(self.pcs[d_num, o_num, :] - self.W[r_num, d_num, :, o_num]).item(), torch.norm(self.pcs[d_num, o_num, :] + self.W[r_num, d_num, :, o_num]).item()))/aux\n",
    "        return(err_mean_pca)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def check_blow_up(self):\n",
    "        return(self.has_blown_up)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def print(self):\n",
    "        print(\"\"); print(\"Parameters for Innerloop_SubspaceNet:\")\n",
    "        print(\"ni=\" + str(self.ni) + \", no=\" + str(self.no) + \" ne=\" + str(self.ne) + \" ns=\" + str(self.ns) + \\\n",
    "              \" nd=\" + str(self.nd) + \" nr=\" + str(self.nr) + \" eta_oja=\" + str(self.eta_oja) + \\\n",
    "              \" eta_eta_antiHebb=\" + str(self.eta_antiHebb) + \" dev=\" + str(self.dev) + \" a_blow=\" + str(self.a_blow) + \\\n",
    "              \" b_blow=\" + str(self.b_blow)); print(\"\")\n",
    "              \n",
    "            \n",
    "            \n",
    "    def info(self):\n",
    "        print(\"\"); print(\"Information on Innerloop_SubspaceNet:\")\n",
    "        print(\"ni input neurons (activity X) projecting on no output neurons (activity Y) with weights W.\")\n",
    "        print(\"Lateral hierarchical inhibition within the output layer (Wlat)\")\n",
    "        print(\"Parallelized on ns, nd and nr (number of different plasticity rules run at the same time)\")\n",
    "        print(\"size(X) = [nr, nd, ns, ni], size(Y) = [nr, nd, ns, no], size(W) = [nr, nd, ni, no], size(Wlat) = [nr, nd, no, no]\")\n",
    "        print(\"Convention: A = [nr, coeffs A_forward + coeffs A_lateral]. Aka with the usual 2nd order expansion A[:,0:26] = coeffs for A_forward and A[:,27:53] = coeffs for A_lat\")\n",
    "        print(\"datasets generated from randomly rotated Gaussian from a diagonal matrix D. datasets fixed at the beginning of meta optimization\"); print(\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2/ Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_hist(A_hist, l_A, n_meta_it):\n",
    "    angle_hist_Oja = np.zeros(n_meta_it)\n",
    "    angle_hist_antiH = np.zeros(n_meta_it)\n",
    "    A_Oja = np.array([0,0,0,0,0,0,0,(-1),0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    A_antiH = np.array([0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    for meta_it in range(n_meta_it):\n",
    "        A_f = A_hist[l_A*meta_it : l_A*meta_it + np.int(l_A/2)]\n",
    "        A_l = A_hist[l_A*meta_it + np.int(l_A/2) : l_A*(meta_it + 1)]\n",
    "        angle_hist_Oja[meta_it] = np.arccos(np.dot(A_f, A_Oja)/((np.linalg.norm(A_f)*np.linalg.norm(A_Oja))))*180/(np.pi)\n",
    "        angle_hist_antiH[meta_it] = np.arccos(np.dot(A_l, A_antiH)/((np.linalg.norm(A_l)*np.linalg.norm(A_antiH))))*180/(np.pi)\n",
    "    return(angle_hist_Oja, angle_hist_antiH)\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_rule_hist(A_hist, l_A, n_meta_it, linewidth = 2, fontsize = 15, font = 'Arial'):\n",
    "    fig, (ax, ax1) = plt.subplots(2, 1, figsize=(2.5, 3), gridspec_kw={'height_ratios': [1, 1]})\n",
    "    for dim_num in range(int(l_A/2)):\n",
    "        col = '#625D5D'\n",
    "        if dim_num == 12:\n",
    "            col = '#4863A0'\n",
    "        elif dim_num == 7:\n",
    "            col = '#fdb462'\n",
    "        ax.plot([A_hist[dim_num + l_A*i].item() for i in range(n_meta_it)], color = col, linewidth = linewidth)\n",
    "    ax.set_ylabel(r'$A$', fontname=\"arial\", fontsize=fontsize , labelpad = 6)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(linewidth)\n",
    "    ax.spines['left'].set_linewidth(linewidth)\n",
    "    ax.tick_params(width=linewidth, labelsize=fontsize, length=2*linewidth)\n",
    "    ax.set_xticks([0,int(n_meta_it/2), n_meta_it])\n",
    "    ax.set_xticklabels([\"\",\"\",\"\"])\n",
    "    ax.set_yticks([-1,0,1])\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontname(font)\n",
    "        \n",
    "    #plot lateral rule\n",
    "    for dim_num in range(int(l_A/2),l_A):\n",
    "        col = '#625D5D'\n",
    "        if dim_num == 39:\n",
    "            col = '#e7298a'\n",
    "        ax1.plot([A_hist[dim_num + l_A*i].item() for i in range(n_meta_it)], color = col, linewidth = linewidth)\n",
    "    ax1.set_ylabel(r'$B$', fontname=\"arial\", fontsize=fontsize , labelpad = 6)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_linewidth(linewidth)\n",
    "    ax1.spines['left'].set_linewidth(linewidth)\n",
    "    ax1.tick_params(width=linewidth, labelsize=fontsize, length=2*linewidth)\n",
    "    ax1.set_xticks([0,int(n_meta_it/2), n_meta_it])\n",
    "    ax1.set_xlabel('meta-iterations', fontsize=fontsize, fontname=\"arial\")\n",
    "    ax1.set_yticks([-1,0,1])\n",
    "    ax1.set_xlim([0,n_meta_it])\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    for tick in ax1.get_yticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_loss_hist(loss_hist, l_A, n_meta_it, linewidth = 2, fontsize = 15, font = \"Arial\"):\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 1.5))\n",
    "    ax.plot(loss_hist, linewidth = linewidth, color = '#d95f02')\n",
    "    ax.set_ylabel(r'$L(A,B)$', fontname=\"arial\", fontsize=fontsize, labelpad = 0)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(linewidth)\n",
    "    ax.spines['left'].set_linewidth(linewidth)\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xticks([0,int(n_meta_it/2), n_meta_it])\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xlabel('meta-iterations', fontsize=fontsize, fontname=\"arial\")\n",
    "    ax.set_ylim([0,np.max(loss_hist)])\n",
    "    ax.tick_params(width=linewidth, labelsize=fontsize, length=2*linewidth)\n",
    "    ax.set_yticks([0,1,2])\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_angle_hist_Oja(angle_hist, l_A, n_meta_it, linewidth = 2, fontsize = 15, font = \"Arial\"):\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 1.5))\n",
    "    ax.plot(angle_hist, linewidth = linewidth, color = '#d95f02')\n",
    "    ax.set_ylabel(r'$A \\measuredangle A^{Oja}$ ' '($^{\\circ}$)', fontname=\"arial\", fontsize=fontsize, labelpad = 0)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(linewidth)\n",
    "    ax.spines['left'].set_linewidth(linewidth)\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xticks([0,int(n_meta_it/2), n_meta_it])\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xlabel('meta-iterations', fontsize=fontsize, fontname=\"arial\")\n",
    "    ax.set_yticks([0,30,60,90])\n",
    "    ax.set_ylim([0,np.max(angle_hist)])\n",
    "    ax.tick_params(width=linewidth, labelsize=fontsize, length=2*linewidth)\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_angle_hist_antiH(angle_hist, l_A, n_meta_it, linewidth = 2, fontsize = 15, font = \"Arial\"):\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 1.5))\n",
    "    ax.plot(angle_hist, linewidth = linewidth, color = '#d95f02')\n",
    "    ax.set_ylabel(r'$B \\measuredangle B^{antiH}$ ' '($^{\\circ}$)', fontname=\"arial\", fontsize=fontsize, labelpad = 0)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(linewidth)\n",
    "    ax.spines['left'].set_linewidth(linewidth)\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xticks([0,int(n_meta_it/2), n_meta_it])\n",
    "    ax.set_xlim([0,n_meta_it])\n",
    "    ax.set_xlabel('meta-iterations', fontsize=fontsize, fontname=\"arial\")\n",
    "    ax.set_yticks([0,30,60,90])\n",
    "    ax.set_ylim([0,np.max(angle_hist)])\n",
    "    ax.tick_params(width=linewidth, labelsize=fontsize, length=2*linewidth)\n",
    "    for tick in ax.get_yticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_fontname(font)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_optimization(outerloop):\n",
    "    angle_hist_Oja, angle_hist_antiH = compute_angle_hist(outerloop.rule_hist, outerloop.lr, outerloop.current_meta_it)\n",
    "    plot_rule_hist(outerloop.rule_hist, outerloop.lr, outerloop.current_meta_it)\n",
    "    plot_loss_hist(outerloop.loss_hist, outerloop.lr, outerloop.current_meta_it)\n",
    "    plot_angle_hist_Oja(angle_hist_Oja, outerloop.lr, outerloop.current_meta_it)\n",
    "    plot_angle_hist_antiH(angle_hist_antiH, outerloop.lr, outerloop.current_meta_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3/ Plasticity rules\n",
    "\n",
    "Convention: A = [nr, coeffs A_forward + coeffs A_lateral]  \n",
    "Aka with the usual 2nd order expansion A[:,0:26] = coeffs for A_forward and A[:,27:53] = coeffs for A_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_forward(A, X, Y, W, dev):\n",
    "    # parametrized plasticity rule for input layer -> output layer weights\n",
    "    # A = coefficients of the self.nr plasticity rules tested in parallel\n",
    "\n",
    "    DW = 0\n",
    "    ct = 0\n",
    "    for x in [torch.ones(X.size(), dtype=torch.double, device=dev), X, torch.mul(X,X)]: # (elementwise mult)\n",
    "        for y in [torch.ones(Y.size(),dtype=torch.double, device=dev), Y, torch.mul(Y,Y)]:\n",
    "            for w in [torch.ones(W.size(),dtype=torch.double, device=dev), W, torch.mul(W,W)]:\n",
    "                DW += torch.einsum(\"r,rdsi,rdso,rdio->rdsio\",A[:,ct],x,y,w)\n",
    "                ct += 1\n",
    "    return(DW)\n",
    "\n",
    "\n",
    "# CAREFUL, assuming the 2 plasticity rules are concatenated in A, 27 coeffs each.\n",
    "def G_lat(A, Y, Wlat, dev):\n",
    "    # parametrized plasticity rule for output layer -> output layer weights\n",
    "    # A = coefficients of the self.nr plasticity rules tested in parallel\n",
    "    \n",
    "    DW = 0\n",
    "    ct = 27\n",
    "    for y_pre in [torch.ones(Y.size(), dtype=torch.double, device=dev), Y, torch.mul(Y,Y)]: # (elementwise mult)\n",
    "        for y_post in [torch.ones(Y.size(),dtype=torch.double, device=dev), Y, torch.mul(Y,Y)]:\n",
    "            for w_lat in [torch.ones(Wlat.size(),dtype=torch.double, device=dev), Wlat, torch.mul(Wlat,Wlat)]:\n",
    "                DW += torch.einsum(\"r,rdso,rdsp,rdop->rdsop\",A[:,ct],y_pre,y_post,w_lat)\n",
    "                ct += 1\n",
    "    return(DW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4/ Training datasets generation\n",
    "#### size(X) = [nr, nd, ns, ni] \n",
    "nr: number of different plasticity rules tested in parallel  \n",
    "nd: number of datasets tested  \n",
    "ns: number of samples per dataset  \n",
    "ni: number of input neurons in the pca network (= number of dimensions of the ambient space of the datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_Gaussian(n_samples, n_datasets, n_plast_rules, n_input, D, dev):\n",
    "    #Xi taken from N(0,transp(Q)*D*Q)\n",
    "    #D diagonal, len(D) must be odd\n",
    "    \n",
    "    X = torch.zeros((n_datasets, n_samples, n_input), dtype=torch.double, device=dev)\n",
    "    for d_num in range(n_datasets):\n",
    "        A = np.random.rand(n_input, n_input)\n",
    "        Q, _ = np.linalg.qr(A)  #generate a random rotation to apply to D \n",
    "        if np.linalg.det(Q) < 0:\n",
    "            Q = -Q\n",
    "        Cov = np.matmul(np.transpose(Q),np.matmul(D, Q)) \n",
    "        for s_num in range(n_samples): \n",
    "            X[d_num, s_num, :] = torch.tensor(np.random.multivariate_normal([0 for i in range(n_input)], Cov))\n",
    "    \n",
    "    #compute & store principal vectors to compare to PCA network performance afterwards\n",
    "    pcs = np.zeros((n_datasets, n_input, n_input)) #which dataset, which pc, which dim\n",
    "    for d_num in range(n_datasets):\n",
    "        pca = PCA()\n",
    "        pca.fit(X[d_num, :, :].to('cpu').numpy())\n",
    "        for i_num in range(n_input):\n",
    "            pcs[d_num, i_num, :] = pca.components_[i_num]\n",
    "    \n",
    "    return(X.repeat(n_plast_rules,1,1,1), torch.tensor(pcs, dtype=torch.double, device=dev))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/ Outer-loop: CMA-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLoop_CMA_ES():\n",
    "       \n",
    "    def __init__(self, outer_loop_params, inner_loop_params):\n",
    "        \n",
    "        # L1 regularization on plasticity rule parameters\n",
    "        self.reg = outer_loop_params[\"reg\"]\n",
    "        \n",
    "        # optimizer parameters\n",
    "        self.lr = len(outer_loop_params[\"A\"])\n",
    "        self.current_meta_it = 0\n",
    "        if outer_loop_params[\"lambd\"] == \"auto\":\n",
    "            self.lambd = int(4 + np.floor(3*np.log(len(outer_loop_params[\"A\"])))) #number of search points in a generation #population size\n",
    "            print(\"generation size lambda = \" + str(self.lambd))\n",
    "        else:\n",
    "            self.lambd = outer_loop_params[\"lambd\"]\n",
    "        inner_loop_params[\"n_rules\"] = self.lambd + 1 #+1 cuz we will test self.m for plotting purpose in addition\n",
    "        self.mu = int(np.floor(self.lambd/2)) #number of non negative recombination weights\n",
    "        wi_aux = np.array([np.log((self.lambd + 1)/2) - np.log(i+1) for i in range(self.lambd)])\n",
    "        self.mu_eff = np.sum(wi_aux[:self.mu])**2/np.sum(np.multiply(wi_aux[:self.mu], wi_aux[:self.mu]))\n",
    "        \n",
    "        # covariance matrix adaptation\n",
    "        self.c_c = (4 + self.mu_eff/self.lr)/(self.lr + 4 + 2*self.mu_eff/self.lr)\n",
    "        self.c_1 = 2/((self.lr + 1.3)**2 + self.mu_eff)\n",
    "        self.c_mu = np.minimum(1 - self.c_1, 2*(self.mu_eff - 2 + 1/self.mu_eff)/((self.lr + 2)**2 + self.mu_eff))\n",
    "        \n",
    "        # selection and recombination\n",
    "        mu_eff_m = np.sum(wi_aux[self.mu+1:])**2/np.sum(np.multiply(wi_aux[self.mu+1:], wi_aux[self.mu+1:]))\n",
    "        alpha_mu_m = 1 + self.c_1/self.c_mu\n",
    "        alpha_mu_eff_m = 1 + 2*mu_eff_m/(self.mu_eff + 2)\n",
    "        alpha_pos_def_m = (1 - self.c_1 - self.c_mu)/(self.lr*self.c_mu)\n",
    "        sum_wi_aux_pos = 0; sum_wi_aux_neg = 0\n",
    "        for i in range(self.lambd):\n",
    "            if wi_aux[i] >= 0:\n",
    "                sum_wi_aux_pos += wi_aux[i]\n",
    "            else:\n",
    "                sum_wi_aux_neg -= wi_aux[i]\n",
    "            \n",
    "        self.wi = np.zeros(self.lambd) #recombination weights\n",
    "        for i in range(self.lambd):\n",
    "            if wi_aux[i] >= 0:\n",
    "                self.wi[i] = wi_aux[i]/sum_wi_aux_pos\n",
    "            else:\n",
    "                self.wi[i] = wi_aux[i]*np.minimum(alpha_mu_m, np.minimum(alpha_mu_eff_m, alpha_pos_def_m))/sum_wi_aux_neg\n",
    "                \n",
    "        # step size control\n",
    "        self.c_sig = (self.mu_eff + 2)/(self.lr + self.mu_eff + 5)\n",
    "        self.d_sig = 1 + 2*np.maximum(0, np.sqrt((self.mu_eff - 1)/(self.lr + 1)) - 1) + self.c_sig\n",
    "        \n",
    "        \n",
    "        # optimizer initialisation\n",
    "        self.C = np.identity(self.lr) #initial covariance matrix\n",
    "        self.m = outer_loop_params[\"A\"] #initial mean distribution\n",
    "        self.sigma = outer_loop_params[\"sigma\"] #step size\n",
    "        self.p_sig= np.zeros(self.lr) # evolution path\n",
    "        self.p_c = np.zeros(self.lr) # evolution path\n",
    "        self.chiN = np.sqrt(self.lr)*(1 - 1/(4*self.lr) + 1/(21*(self.lr**2))) #expectation of ||N(0,I)||\n",
    "        self.innerloop = Innerloop_SubspaceNet(inner_loop_params)\n",
    "        \n",
    "        # for plotting\n",
    "        self.rule_hist = outer_loop_params[\"A\"] #plot best element of population at each meta-iteration\n",
    "        self.loss_hist = []  #plot loss assiociated to best element in the population\n",
    "        self.sigma_init = outer_loop_params[\"sigma\"]\n",
    "\n",
    "        \n",
    "        \n",
    "    def score_population(self, x): #evaluate meta-objective at the points in parameter space in A_pop \n",
    "        loss_no_l1 = self.innerloop.score(x)\n",
    "        l1 = np.multiply([np.linalg.norm(x[i, :],ord = 1).item() for i in range(self.lambd)],self.reg)\n",
    "        return(loss_no_l1[-1], loss_no_l1[:-1] + l1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def run(self, n_meta_it): #CMA-ES optimizer\n",
    "        initial_meta_it = self.current_meta_it # meta-it is the generation number\n",
    "        \n",
    "        while self.current_meta_it < (initial_meta_it + n_meta_it):   \n",
    "            \n",
    "            # eigendecomposition of C: C = BDDt(B) \n",
    "            D,B = np.linalg.eigh(self.C)\n",
    "            D = np.diagflat(np.sqrt(D))\n",
    "            \n",
    "            # sample new population of search points\n",
    "            z = np.array([np.random.multivariate_normal([0 for i in range(self.lr)], np.identity(self.lr)) for j in range(self.lambd)])\n",
    "            y = np.array([np.matmul(B, np.matmul(D, z[i,:])) for i in range(self.lambd)])\n",
    "            x = self.m + self.sigma*y #x are the new population of rules for this generation (i.e. meta-iteration)\n",
    "            \n",
    "            # sort population by ascending score\n",
    "            x_aux = np.concatenate((x, np.expand_dims(self.m, axis=0))) #get loss(m) for plotting purposes only, m itself not included as an individual in CMA-ES\n",
    "            loss_current, meta_obj = self.score_population(x_aux)\n",
    "            x_sorted = np.array([x for _,x in sorted(zip(meta_obj, x), key=lambda pair: pair[0])]) \n",
    "            y_sorted = (x_sorted - self.m)/self.sigma\n",
    "            \n",
    "            # selection and recombination\n",
    "            y_w = np.sum(np.array([self.wi[i]*y_sorted[i,:] for i in range(self.mu)]), axis = 0) #step of the distribution mean\n",
    "            self.m = self.m + 1*self.sigma*y_w #cm = 1, see tuto\n",
    "            \n",
    "            # step size control\n",
    "            d_1 = np.diagflat([1/i for i in np.diagonal(D)])\n",
    "            C_12 = np.matmul(B, np.matmul(d_1, np.transpose(B)))\n",
    "            self.p_sig = (1 - self.c_sig)*self.p_sig + np.sqrt(self.c_sig*(2 - self.c_sig)*self.mu_eff)*np.matmul(C_12, y_w)\n",
    "            self.sigma = self.sigma*np.exp((self.c_sig/self.d_sig)*((np.linalg.norm(self.p_sig)/self.chiN) - 1))\n",
    "            \n",
    "            # heaviside function for step size control\n",
    "            if np.linalg.norm(self.p_sig)/np.sqrt(1 - (1 - self.c_sig)**(2*(self.current_meta_it + 1))) < \\\n",
    "            (1.4 + 2/(self.lr + 1))*self.chiN:\n",
    "                h_sig = 1\n",
    "            else:\n",
    "                h_sig = 0\n",
    "            \n",
    "            # covariance matrix adaptation\n",
    "            self.p_c = (1 - self.c_c)*self.p_c + h_sig*np.sqrt(self.c_c*(2 - self.c_c)*self.mu_eff)*y_w\n",
    "            wi_o = np.zeros(self.lambd)\n",
    "            for i in range(self.lambd):\n",
    "                if self.wi[i] > 0:\n",
    "                    wi_o[i] = self.wi[i]\n",
    "                else:\n",
    "                    wi_o[i] = self.wi[i]*self.lr/(np.linalg.norm(np.matmul(C_12, y_sorted[i,:])))**2\n",
    "            delta_h_sig = (1 - h_sig)*self.c_c*(2 - self.c_c)\n",
    "            self.C = (1 + self.c_1*delta_h_sig - self.c_1 - self.c_mu*np.sum(self.wi))*self.C \\\n",
    "            + self.c_1*np.outer(self.p_c, self.p_c) \\\n",
    "            + self.c_mu*np.sum([wi_o[i]*np.outer(y_sorted[i,:], y_sorted[i,:]) for i in range(self.lambd)], axis = 0)\n",
    "\n",
    "            # keep user informed of meta-optimization progress\n",
    "            if (self.current_meta_it % 10 == 0):\n",
    "                print(\"iteration \" + str(self.current_meta_it+1) + \"/\" + str(initial_meta_it + n_meta_it))\n",
    "                print(\"current_loss (without L1 term): \" + str(loss_current))\n",
    "                print(\"m \" + str(self.m))\n",
    "                print(\"sigma \" + str(self.sigma))\n",
    "    \n",
    "            self.current_meta_it += 1  \n",
    "            self.rule_hist = np.concatenate((self.rule_hist, self.m),0)\n",
    "            self.loss_hist.append(loss_current)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def print(self):\n",
    "        print(\"\"); print(\"Parameters for Outerloop_CMA_ES:\")\n",
    "        print(\"lambd=\" + str(self.lambd) + \" sigma init=\" + str(self.sigma_init) + \" reg=\" + str(self.reg)); print(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    def info(self):\n",
    "        print(\"\"); print(\"Information on Outerloop_CMA_ES:\")\n",
    "        print(\"Outer loop using the evolutionnary strategy CMA_ES. Based on The CMA Evolution Strategy: A Tutorial, Nikolaus Hansen, arXiv 2016\")\n",
    "        print(\"At each generation, new individuals (here plasticity rules) are drawn from N(m,C). Both m (distr mean) and C (covariance matrix) are learnt\")\n",
    "        print(\"Specific requirements for innerloop: none\")\n",
    "        print(\"Important parameters:\")\n",
    "        print(\"lambda: generation size. Can be set automatically depending on n_coeffs of the plasticity rules, but this number is often too small\")\n",
    "        print(\"sigma: initial step size for both m and C. sigma is adapptative, but initial value should be ~0.3(a-b) if the the estimated search range for the plasticity rules coeffs is [a,b]**lr\")\n",
    "        print(\"reg: coefficient for L1 regularization\")\n",
    "        print(\"There are many other parameters, most of them have a strongly recommended value (see tutorial). Go inside code to change them\"); print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3/ Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/3\n",
      "current_loss (without L1 term): 0.8966047279170606\n",
      "m [-0.00998656 -0.01654062 -0.02325317 -0.02174401 -0.01262745 -0.01467147\n",
      " -0.03893013  0.00545167  0.01615412 -0.03589857  0.03659769  0.00473748\n",
      " -0.01463013 -0.01900919  0.01206072  0.00263488 -0.00263614  0.00315443\n",
      " -0.0417886  -0.02027498 -0.0278347  -0.07143004 -0.00326899 -0.00375575\n",
      " -0.00726704  0.00477398 -0.00407085 -0.0446319   0.02635564 -0.03746088\n",
      " -0.03696363 -0.03078356 -0.01839898 -0.00140438  0.01086957 -0.00185699\n",
      "  0.01609541  0.00322876  0.01554163  0.01413769  0.00409343 -0.04632359\n",
      " -0.03305811 -0.02324121  0.00571362  0.03545387 -0.01149886  0.0004083\n",
      " -0.0500096   0.03403292  0.00563779 -0.04522343 -0.01710909  0.00145847]\n",
      "sigma 0.09283066139483115\n"
     ]
    }
   ],
   "source": [
    "##### innerloop_params for Innerloop_SubspaceNet (or ICANet) and initial plasticity rule ######\n",
    "D3 = np.diag([1,0.8,0])\n",
    "#To reproduce results from fig 2 use these matrices and change n_input accordingly\n",
    "D5 = np.diag([1,0.8,0.6,0.4,0.2])\n",
    "D50 = np.zeros((50,50)); D50[0,0] = 1; D50[1,1] = 0.9; D50[2,2] = 0.8; D50[3,3] = 0.7; D50[4,4] = 0.6; D50[5,5] = 0.5\n",
    "D_chosen = D3; n_input = len(D_chosen)\n",
    "n_output = 2; n_samples_per_dataset = 200; n_epochs = 1000; n_datasets = 10;\n",
    "eta_oja = 1/5; eta_antiHebb = 1/5; a_blow = 1; b_blow = 1;\n",
    "## initial plasticity rule for outerloop\n",
    "As_Oja_AntiH = np.array([0,0,0,0,0,0,0,(-1),0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "A_oja = np.array([0,0,0,0,0,0,0,(-1),0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "A_antih = np.array([0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "A_rand = np.random.normal(loc = 0, scale =0.001, size=len(As_Oja_AntiH))\n",
    "A_chosen = A_rand\n",
    "\n",
    "inner_loop_params = dict([('n_samples_per_dataset', n_samples_per_dataset), ('D', D_chosen), ('n_output', n_output), \\\n",
    "('n_epochs', n_epochs), ('n_datasets', n_datasets), ('n_input', n_input), ('eta_oja', eta_oja), \\\n",
    "('eta_antiHebb', eta_antiHebb), ('a_blow', a_blow), ('b_blow', b_blow)])\n",
    "\n",
    "\n",
    "\n",
    "##### for innerloop Innerloop_SubspaceNet #####\n",
    "n_meta_it = 3; reg = 0.1; sigma = 0.1; lambd = 50\n",
    "\n",
    "outer_loop_params = dict([('A', A_chosen),  (\"sigma\", sigma), (\"reg\", reg), (\"lambd\", lambd)])\n",
    "\n",
    "\n",
    "##### Start the simulation #####\n",
    "outerloop = OuterLoop_CMA_ES(outer_loop_params, inner_loop_params)\n",
    "start = time.time()\n",
    "outerloop.run(n_meta_it) \n",
    "print(\"Execution took \" + str(np.round(time.time()-start,2)) + \"s\")\n",
    "plot_optimization(outerloop)\n",
    "outerloop.print()\n",
    "outerloop.info()\n",
    "outerloop.innerloop.info()\n",
    "outerloop.innerloop.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
